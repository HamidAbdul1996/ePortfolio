<!DOCTYPE HTML>
<!--
-->
<html>
	<head>
		<title>Module Three</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>Hamid Abdul</strong> <span></span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="aboutme.html">about me</a></li>
							<li><a href="eportfolio.html">ePortfolio</a></li>	
						</ul>
						<ul class="actions stacked">
							<li><a href="#" class="button primary fit">Get Started</a></li>
							<li><a href="#" class="button fit">Log In</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1> Deciphering Big Data </h1>
									</header>
									<h2>In this module, we’ll explore the diversity of different data types, sources and methods of data collection. You will also develop an understanding of the process of data wrangling from collection to publishing and the ability to identify resources to undertake the data wrangling process. These processes are essential because it improves the quality of data by transforming the data collected in the raw state to a more usable format for a variety of applications by eliminating uneccessary duplications and redundancies of data items through processes such as cleansing and normalisation.</h2>
									
									<h2> Summarising this modules studies: </h2> 
									<ul style=“list-style-type:square”>

<li> In unit one, we explored big data, associated technologies and concepts underlying data management and how these technologies powerfully manipulate big data in an organisation. We also reviewed the features and characteristics of big data. </li>

<li> In unit two, we examined different data types and formats and looked at how these impact on the way data is stored with particular emphasis on data file formats. We also analysed unstructured data formats and how to enable APIs to parse data.</li>

<li> In unit three, we explored various sources of data, data collection methods, and evaluated the issues and challenges that come with the collection process. </li>

<li> In unit four, we explored concepts, techniques, and methods of data cleaning and transformation. We also looked at the requirements for the data design and process automation. </li> 

<li> In unit five,  we looked at the practical applications of data cleaning using Python. We also looked into database representation and architecture. </li>

<li> In unit six, we looked at database design and normalisation, paying particular attention to the formation of a relational database and how its constructed and broked down into a normalised method for the storange and retrieval of data. </li>

<li> In unit seven, we looked at how to construct normalised tables and examined data attributes, associations, operations and relationships. </li>

<li> In unit eight, we looked at compliance frameworks with regards to data management in respect of rights of individuals and obligations of organisation to stakeholders. </li>

<li> In unit nine, we explored concepts and theories underlying Database Management Systems. This comprised of flat files, relational databases and non relational databases. </li>

<li> In unit ten, we analysed and evaluated APIs and looked into how APIs can faciliate data parsing and inter process communication.</li>

<li> In unit eleven, we looked at how database systems can fail and what systems are in place to guarantee system failures do not lose dataor interfere with the integrity of the data within the system.  </li
																										   
<li> In unit twelve, we explored the role of machine learning and how that would drive the big advances of big data analysis. We also looked further into compliance frameworks. </li>
									
									</ul>
									<h2><u>Artefacts</u></h2>
									<h2>1. Collaborative Discussion 1 - The Data Collection Process</h2> 
									<h3>Learning Outcomes </h3>
									<ul>

									<li>Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling. </li>

									<li>Critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques (involving preparing, cleaning, exploring, creating, optimising and evaluating big data) to solve them.</li>

									</ul>
									<h3>Initial Post</h3>
									<p> The Internet of Things (IoT) has revolutionised the way we interact with our surroundings by enabling the collection of extensive data from a multitude of connected devices. With its transformative potential, the IoT has the capacity to revolutionise industries and enhance various aspects of our daily lives.

In contrast to conventional data collection methods that typically rely on sporadic or manual gathering, IoT devices offer a continuous flow of data. This real-time stream of information empowers organizations to make prompt and well-informed decisions, enabling them to swiftly adapt to evolving circumstances and base their actions on data-driven insights (Kumar et al, 2019).

Moreover, the IoT enables the collection of a wide range of data types, encompassing numerical measurements, texts, images, audio, video, and beyond. This diverse array of data empowers multidimensional analysis, enabling the exploration of correlations and patterns that may remain concealed when relying solely on traditional data sources (Abu-Elkheir et al, 2013).

However, the IoT also introduces inherent risks and challenges when it comes to large-scale data collection. IoT devices have the capacity to collect and transmit sensitive information, including personal and confidential data. The interconnected nature of IoT systems, coupled with potential vulnerabilities in devices, can make them susceptible to cyber-attacks, potentially resulting in privacy violations and breaches of confidentiality (Kumar et al, 2019).

Furthermore, the collection of IoT data heavily depends on sensors and devices that are susceptible to errors or malfunctions. Depending on flawed data can result in inaccurate analysis, flawed decision-making, and potential risks to operational processes. It is crucial to address and mitigate these challenges to ensure the reliability and integrity of the collected data for effective utilisation (Kumar et al, 2019).

Balancing the opportunities of IoT data collection with these risks necessitates robust security measures, stringent privacy protection, and the mitigation of potential errors in data collection. Organisations and individuals must prioritise data integrity, privacy, and ethical considerations when deploying and utilising IoT technologies.</p>

<p><b>
References </b></p>
									
<p> Kumar, S, Tiwari, P. & Zymbler, M. (2019) Internet of Things is a revolutionary approach for future technology enhancement: a review. Journal of Big Data. 6, 111. DOI: https://doi.org/10.1186/s40537-019-0268-2 </p>

<p>Abu-Elkheir, M, Hayajneh, M. & Abu Ali, N. (2013) Data Management for the Internet of Things: Design Primitives and Solution, Sensors. 13(11): 1182-612. DOI: 10.3390/s131115582.</p>


									<h3> Summary post </h3>

									<p> Upon reflecting on my initial post and considering the insightful contributions from my peers, I have gained a broader perspective on the challenges, issues, risks, limitations, and opportunities associated with data collection and the Internet of Things (IoT). Below, I will summarise the main points discussed:</p>

<p>Oppurtunities:</p>
<ul>
<li>The IoT has revolutionised our interaction with the environment by enabling the collection of diverse data from numerous devices. This continuous flow of data empowers organisations to make prompt and well-informed decisions, adapting to evolving circumstances based on data-driven insights (Kumar et al., 2019).</li>

<li>IoT facilitates the collection of various data types, including numerical measurements, texts, images, audio, video, and more Abu-Elkheir et al, 2013.</li>
</ul>
<p>Challenges:</p>
<ul>
<li>Large-scale data collection through the IoT raises inherent concerns regarding privacy and data security. As mentioned in our posts, potential solutions to address these concerns include regulatory compliance, user control and access, and industry collaboration.</li>

<li>Sensors and devices used in IoT systems are susceptible to errors or malfunctions, leading to inaccurate data analysis. External factors can also compromise data integrity, resulting in the need for costly data cleaning.</li>
</ul>
<p>One of my peers astutely pointed out the necessity of adapting data architecture models for IoT. Three examples of adaptable data architecture models are Kappa, Lambda, and Cloud Computing architecture. Notably, Kappa architecture has already been tailored for smart farming applications (Penka et al, 2021).

Achieving a balance between the opportunities presented by IoT data collection and the associated risks requires implementing robust security measures, stringent privacy protection, and effective mitigation strategies for potential data collection errors. It is essential for organisations and individuals to prioritise data integrity, privacy, and ethical considerations when deploying and utilising IoT technologies.</p>

<p><b>
References </b></p>

<p> Penka, J, B, N., Mahmoudi, S & Debauche, O. (2021) A New Kappa Architecture for IoT Data Management in Smart Farming, Procedia Computer Science. 191: 17-24. DOI: https://doi.org/10.1016/j.procs.2021.07.006</p>
<p> Kumar, S, Tiwari, P. & Zymbler, M. (2019) Internet of Things is a revolutionary approach for future technology enhancement: a review. Journal of Big Data. 6, 111. DOI: https://doi.org/10.1186/s40537-019-0268-2</p>
<p> Abu-Elkheir, M, Hayajneh, M. & Abu Ali, N. (2013) Data Management for the Internet of Things: Design Primitives and Solution, Sensors. 13(11): 1182-612. DOI: 10.3390/s131115582</p>

<h3>Reflection</h3>
									<p> Reflecting on the collaborative discussion and the learning outcomes, I believe I have increasingly recognised the importance of striking a balance between the opportunities provided by IoT and the inherent challenges that come with such interconnected data collection systems. My posts and discussion with my peers have allowed me to identify and manage a range of challenges, security issues, risks, limitations, and opportunities with regard to data wrangling. 
Through the analysis of data collection via IoT, I identified significant security concerns arising from potential vulnerabilities in IoT devices and systems. These risks are paramount due to the sensitivity of the data collected, posing threats to privacy and confidentiality. I further recognised the susceptibility of IoT devices to errors and malfunctions, which could compromise data integrity and lead to flawed decision-making. By managing these challenges, we can ensure the data's reliability and facilitate its effective utilisation, thus delivering insightful, data-driven solutions. Simultaneously, the potential benefits of IoT data collection were not overlooked. IoT enables the collection of a broad array of data types in real-time, enhancing decision-making capabilities and enabling multidimensional data analysis. In doing so, it encourages the uncovering of patterns and correlations that may remain hidden with traditional data sources, thereby fostering the generation of innovative solutions to complex problems.
My summary post demonstrates my capability to critically analyse data wrangling issues and identify suitable solutions. I underscored the importance of regulatory compliance, user control, and industry collaboration, demonstrating an understanding of the need for robust security and privacy measures in the IoT context. Additionally, my knowledge of data systems optimisation was evidenced through the suggestion of adaptable data architecture models like Kappa, Lambda, and Cloud Computing for IoT usage.
</p>

									
									<h3>2. Web scraping</h3>

									<h3> Learning Outcomes </h3>
									<ul>
										<li>Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling.</li>
										<li>Critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques (involving preparing, cleaning, exploring, creating, optimising and evaluating big data) to solve them.</li>
										<li>Systematically develop and implement the skills required to be effective member of a development team in a virtual professional environment, adopting real life perspectives on team roles and organisation. </li>
									</ul>
									 <p> The web scraping script below shows how I scraped all instances of the term "data scientist" from the Data Science Wikipedia page. After capturing these references, the sentences containing them were printed and stored in an XML file. The task utilised tools like Beautiful Soup and Requests modules. Requests smoothly handled HTTP requests, quickly fetching HTML content from the Wikipedia page. Meanwhile, Beautiful Soup simplified parsing this HTML content, transforming the complex HTML into Python objects like tags and comments. Consequently, this enabled efficient identification of sentences containing "data scientist". Employing these modules together facilitated effective and efficient data extraction from the webpage, notably improving the speed and reliability of my work. These tools, hence, proved essential for web scraping and data analysis.</p>
									<p></p><a href = "https://github.com/HamidAbdul1996/E-Portfolio_tasks/blob/main/WebScraping.ipynb"> Download Jupyter Notebook file</a></p>
									<p> <img src ="images/Web scraping script.jpg"</p>

									<h3>Reflections</h3>
									<p> I found the task of web scraping to be quite challenging. While I have a basic understanding of HTML and grasped the overall concept, I struggled with extracting data from more complex websites. Specifically, I attempted to scrape data scientist roles from Indeed but encountered difficulties. Upon reflection, I believe the structure and layout of Indeed's website played a significant role in hindering my progress. The presence of JavaScript, which Beautiful Soup has limitations with, made it even more challenging. In hindsight, I now realise that Indeed has a public API available, which would have been a more suitable approach. Additionally, I suspect that Indeed implemented anti-scraping measures, as I came across information about this during my research. Although I had difficulties, I found the entire task very useful and it helped me develeop my skills looking at the code on external websites and how they are structured. I eventually did a simpler web scraping task on Wikipedia. 
										Upon reflection, web scraping can serve as a gateway to unlocking a wealth of information from the realms of the web. This process of extracting data empowers us to gain valuable insights, automate tasks, and make informed decisions. However, its important to note the ethical considerations intertwined with this practice. Respecting the terms of service of websites, being mindful of server load, and safeguarding the privacy of personal data are essential aspects to ponder. Moreover, one cannot overlook the legal implications associated with web scraping, such as potential infringement on intellectual property rights or violation of data protection laws. Engaging in responsible and conscientious web scraping practices is paramount for a fruitful and lawful exploration of the online landscape.
</p>
									<h3> 3. Data Cleaning  </h3> 
									<h3> Learning Outcomes</h3>
									<ul>
										<li>Clean and transform data using the data pipeline as a guide.</li>
										<li> Examine and understand factors that affect data cleaning.</li>
										<li>Understand requirements for design automation.</li>
									</ul>

									<h3> 4. Normalisation and Data Build Task </h3>
									<h3> Learning Outcomes </h3>
									<ul>
										<li> Critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques (involving preparing, cleaning, exploring, creating, optimising and evaluating big data) to solve them.</li>
										<li> Design, develop and evaluate solutions for processing datasets and solving complex problems in various environments using relevant programming paradigms.</li>
										<li> Systematically develop and implement the skills required to be effective member of a development team in a virtual professional environment, adopting real life perspectives on team roles and organisation.</li>
										
									</ul>

									<h3> Normalisation Task </h3>

									<p> Our task is to normalise the table below. </p>
									<p> <img src ="images/Normalisation Task.jpg"></p>
									<p> To satisfy the first normal form, every value should be atomic and each row should be unique. There must also be a primary key (unique identifier) for each set of related data. We have creted a new unique identifier called 'Exam ID' so we can split this into two tables, a Student table and an StudentExam table. In the Student table the primary key is 'Student Number' and in the StudentExam table, it is 'Exam ID'. </p>
									<p> <img src ="images/Normalisation 1nS.jpg" alt = "Student Table"></p>

									<p> <img src ="images/Normalisation 1n StudentExam.jpg" alt= "StudentExam Table"></p>

									<p> To satisfy the second normal form, the table must already be in the first form and every column that is not part of the primary key should be dependent on the primary key. In other words, their is no partial dependency. In our case, in the StudentExam table, 'Exam Score' is related to the student which means it is not fully dependant on 'Exam ID' which violates the second norm. To resolve this, we can split the StudentExam table into two tables to maintain the relationship between students and courses. We will split this into two tables, one called StudentCourse and one called Exam.</p>
									<p> <img src ="images/Normalisation 2n SC.jpg" alt = "StudentCourse Table"></p>
									<p> <img src ="images/Normalisation 2n E.jpg" alt = "Exam Table"></p>

									<p> Finally to satisfy the third normal form, the table must already be in the second form that every column that is not part of the primary key can only be dependent on the primary key itself, in other words there can be no transitive dependencies. In our case, in the Exam table, we can see 'Teacher Name' is dependent on 'Exam Boards'. This is a transitive dependency. To resolve this, we need to move 'Exam Boards' and 'Teacher Name' into a seperate table.</p>
									<p> <img src ="images/Normalisation 3n UE.jpg" alt = "Updated Exam Table"></p>
									<p> <img src ="images/Normalisation 3n B.jpg" alt = "Boards Table"></p>
									<p> The data has now been normalised, the four final tables would be the Student Table, the StudentCourse Table, the Updated Exam Table and the Boards Table.</p>

									

			

									<h3>Reflections</h3>
									 <p> The normalisation and data construction task played a significant role in my progression within this module, specifically with regard to the learning outcomes. Its application was particularly beneficial when we were tasked with creating a relational database for the executive summary of our Unit 11 assessment. I've uploaded a file that showcases the relational database we created for this assessment, demonstrating the practical implementation of these same principles. The tables within this database have been effectively organised, with primary and foreign keys allocated, and compliance with all three normalisation forms, demonstrating my comprehension and ability to apply these principles.
Upon reflection, it's clear to me that normalisation is a pivotal aspect of the data wrangling process. It aids in cleansing and transforming raw data into a more practical format. Normalisation also helps in structuring data in such a way that it diminishes redundancy and enhances data integrity. By grasping the concept of normalisation, I believe I am better equipped to identify and rectify data anomalies, as well as construct more organised databases. Moreover, normalisation facilitates improved querying of databases and serves as the foundation for efficient data processing solutions.
</p>

									<h3> Data Build Task </h3>
								<p><a href = "https://github.com/HamidAbdul1996/E-Portfolio_tasks/blob/main/DataBase%20Build.sql"> Download MYSQL file</a></p>	
									
									<h3> 5. Collaborative Discussion 2 - Comparing Compliance Laws </h3> 
									<h3> Learning Outcomes </h3>
									<ul>
										<li> Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling.</li>

									</ul>
									<h3> Intial Post</h3>
									<p> 
The General Data Protection Regulation (GDPR) is a legal framework that governs data protection and privacy across the European Union and the European Economic Area. The Information Commissioner's Office (ICO) is an independent non-departmental public entity in the UK, advocating for information rights in the public interest. The ICO enforces the data protection laws in the UK, which includes the GDPR and the Data Protection Act 2018 which was later modified to the Data Protection Act 2019 following Brexit.

The General Data Protection Regulation (GDPR) and the Information Commissioners Office in the UK are similar in that they seek to protect personal data and ensure its secure handling. Both require organisations to implement suitable technical and organisational measures to guarantee the security of personal data, as stipulated in GDPR's Article 32 and the ICO's 'Security' principle. These requirements obligate data controllers and processors to prevent unauthorised or unlawful processing, accidental loss, destruction, or damage of data, with the ICO providing more detailed guidelines to support practical implementation (ICO, 2023 & NCSC, 2023).

Despite these common goals, there are significant differences. The GDPR presents a broad and flexible framework, enabling businesses to select security measures based on a risk-based approach (Quelle, 2018). Conversely, the ICO provides a more detailed guide, amplifying the practical implementation of security measures (ICO, 2023). Moreover, their exemptions differ: the GDPR's Article 23 permits restrictions under certain conditions such as national security, while the ICO recognises fewer, more specific exemptions (Staunton et al, 2019). Unless the data processing is for law enforcement purposes, most ICO exemptions are tied to specific rights rather than general principles. Therefore, although the GDPR and the ICO share the paramount goal of personal data security, their approaches, practical applications, and acknowledged exemptions vary.</p>
									

									<p><b> References </b></p>

									<p> ICO (2023) A guide to data security. Available from: https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/security/a-guide-to-data-security/. [Accessed 18th July 2023].</p>
									<p>ICO (2023) Data protection by design and default. Available from: https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/guide-to-accountability-and-governance/accountability-and-governance/data-protection-by-design-and-default/. [Accessed 18th July 2023] </p>
									<p> National Cyber Security Centre (2023) General Data Protection Regulation. Available from: https://www.ncsc.gov.uk/information/gdpr. [Accessed 18th July 2023] </p>
									<p>Quelle, C. (2018) Enhancing Compliance under the GDPR: The Risky Upshot of the Accountability – and Risk Based Approach. European Journal of Risk Regulation. 9(3): 502-526, DOI: 10.1017/err.2018.47 </p>
									<p> Staunton, C., Slokenerga, S., Mascalzoni, D. (2019) The GDPR and the research exemption: considerations on the necessary safeguards for research biobanks. European Journal of Human Genetics. 27L 1159-1167. DOI: https://doi.org/10.1038/s41431-019-0386-5</p>

									<h3> Summary Post </h3>
									<p> 
Upon reflecting on my initial post and considering the insightful contribution of my peers, I have gained a broader understanding of GDPR and ICO in relation to the securing of personal data whilst also looking at similarities and differences in a range of other countries.

It is interesting to observe the variations in the roles of data processors and data controllers across different countries. In the UK the roles are defined under the framework provided by the GDPR and DPA. Under the GDPR and the ICO, a data processor is responsible for processing personal data on behalf of a data controller, adhering to the controller's instructions, implementing security measures, and potentially assuming legal obligations outlined in a contractual agreement (GDPR, 2023 & ICO, 2023) . Conversely, the NDPA in Nigeria defines a data processor in a similar manner to the GDPR, but with less extensive provisions. The NDPA places greater emphasis on the responsibilities and obligations of data controllers (One Trust Guidance, 2021).

Likewise, the GDPR and ICO define a data controller as an entity or individual that determines the purposes and methods of personal data processing, carrying primary responsibility for ensuring compliance with the GDPR's principles and requirements. The NDPA also defines a data controller as an entity or individual with similar responsibilities. However, specific obligations and requirements for data controllers may differ between the GDPR and the NDPA. It’s also interesting to note that some countries do not require a data protection officer such as Switzerland (FDAP).

Another interesting observation is the extraterritorial application of the GDPR, which extends its reach to organisations outside the EU that process the personal data of EU residents. In contrast, the NDPA or the LFDPPP in Mexico does not possess this extraterritorial scope. Furthermore, the GDPR applies to all entities involved in processing or controlling personal data, whereas the PIPEDA in Canada solely applies to commercial use, excluding public bodies. These would fall under the provincial Freedom of Information and Privacy Acts. Thus, the scope and application of the GDPR is comparatively very broad (Granmar, 2021).

In terms of enforcement, it is notable that while the GDPR imposes substantial fines as a deterrent (€20 million or 4% of the global annual turnover), other compliance laws have more varied enforcement mechanisms. For instance, while there are stringent fees, Germany (BDSG) also follows a tiered structure for penalties, Mexico (LFPDPPP) utilises administrative sanctions, and Switzerland (FDAP) imposes fines on individuals rather than corporations.

  </p>


									<p><b> References</b></p>
									<p> GDPR (2023) Data Controllers and Processors. Available from: https://www.gdpreu.org/the-regulation/key-concepts/data-controllers-and-processors/. [Accessed 19th July 2023] </p>
									<p> Granmar, C.G. (2021) Global applicability of the GDPR in context. International Data Privacy Law. 11(3): 225-244. DOI: https://doi.org/10.1093/idpl/ipab012 </p>
									<p> ICO (2023) A guide to data security. Available from: https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/security/a-guide-to-data-security/. [Accessed 18th July 2023]. </p>
									<p> One Trust Guidance (2021) Comparing Privacy Laws: GDPR vs Nigeria Data Protection Regulation. Available from: https://www.dataguidance.com/resource/comparing-privacy-laws-gdpr-v-nigerian-data, [Accessed 19th July 2023].</p>




									<h3> Reflections</h3>
									<p> Reflecting on the post and ensuing discussion concerning the GDPR, ICO, and data security legislations of various countries, I have not only expanded my understanding of data protection regulations but also of the challenges, security issues, and risks involved in data wrangling. The examination of the roles of data processors and controllers, as well as their differing responsibilities across various jurisdictions, highlighted the challenges of ensuring compliance in a global context. Being aware of such nuances enables me to better manage data while respecting local laws and limitations, a crucial aspect in data wrangling. The extraterritorial scope of GDPR underscored the complexities in managing data of EU residents for organisations outside the EU. This fosters awareness of security issues and risks associated with data handling and privacy regulations, enabling me to anticipate potential problems and implement safeguards accordingly. In conclusion, this discussion has underscored the importance of remaining vigilant about international data protection regulations while wrangling data.</p>

									
								
										
									<h3> 6. API Security Requirements </h3>
									<h3> Learning Outcomes </h3>
									<ul>
										<li>Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling. </li>
										<li> Critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques (involving preparing, cleaning, exploring, creating, optimising and evaluating big data) to solve them. </li>
										<li> Systematically develop and implement the skills required to be effective member of a development team in a virtual professional environment, adopting real life perspectives on team roles and organisation. </li>
									</ul>
									<p> Spotify API Security Requirements Specification
For my evaluation, I have selected the Spotify Web API due to its extensive capabilities. This API provides applications with an interface to interact with Spotify's music streaming service, enabling access to a diverse range of resources, such as Spotify's music catalogue, playlist management, and even playback control. However, despite its functionality, it is not immune to potential threats. Unauthorised access, privilege escalation, Cross-Site Request Forgery (CSRF), Cross-Site Scripting (XSS), token hijacking, Denial of Service (DoS), and data breaches remain significant concerns that warrant comprehensive security measures.

At the heart of these measures lies the concept of authorisation. With Spotify API, each request necessitates valid client credentials, including a client ID and a secret. Spotify capitalises on the versatility of OAuth2, employing its four grant types for access token requests. In this context, the authorisation code flow is especially beneficial for long-lasting applications, such as web and mobile apps. Ensuring tokens are securely stored, separate from client-side code or logs, can effectively thwart attempts at unauthorised access and privilege escalation (Spotify Developer, 2023).

When it comes to external data, it is imperative to conduct thorough cleansing to mitigate risks like injection, XSS, and DoS attacks. SQL interactions mandate the use of parameterised queries, a critical tool in preventing SQL injection attacks, thus treating user input as data, not executable code (Spotify Developer, 2023).

Data encryption is non-negotiable when transmitting sensitive data over the network. The HTTPS protocol is a reliable choice, offering robust encryption for data in transit. API keys and other sensitive data must be stored securely, utilising methods such as Python-dotenv for managing environment variables, or even opting for a separate Python file (Spotify Developer, 2023).


Further mitigation strategies incorporate the use of the secure OAuth 2.0 protocol for user authentication and authorisation. In addition, validating and sanitising user inputs is crucial to fend off XSS attacks. All interactions with the Spotify API should be conducted over HTTPS, fortifying data during transit and minimising the risk of token hijacking. Periodic security audits are strongly recommended to promptly identify and rectify any vulnerabilities (Spotify Developer, 2023).

Conforming to Spotify's Terms of Service, privacy policy, and pertinent laws is a prerequisite for data sharing. The use of secure encryption methods, particularly over public networks, is advised. Given that Spotify's Terms of Service typically frown upon web scraping, data acquisition should be facilitated via the official API, mindful of rate limits and access scopes. Importantly, any data procured from Spotify should be stored securely, with access granted only to authorised personnel.
</p>
									<p><b>References</b></p>
									

									<p>Spotify Developer (2023) Authorisation. Available from: https://developer.spotify.com/documentation/web-api/concepts/authorization. [Accessed on 20th July 2023]. </p>
									<p>Spotify Developer (2023) Authorisation Code Flow. Available from: https://developer.spotify.com/documentation/web-api/tutorials/code-flow. [Accessed on 20th July 2023]. </p>
									<p>Spotify Developer (2023) API calls. Available from: https://developer.spotify.com/documentation/web-api/concepts/api-calls. [Accessed 20th July 2023]. </p>
									<p> Spotify Developer (2023) Web API. Available from: https://developer.spotify.com/documentation/web-api. [Accessed 20th July 2023].</p>
									<p> Spotify Developer (2023) Compliance Tips. Available from: https://developer.spotify.com/compliance-tips. [Accessed 20th July]. </p>

									<h3> Reflections</h3>

									<p> 
The evaluation of the Spotify Web API has significantly contributed to me achieving the learning outcomes, specifically around the various tools that can be used to analyse data wrangling problems. The evaluation specifically highlighted the importance of comprehensive security measures in working with APIs. I have gained an understanding of potential threats such as unauthorised access, privilege escalation, CSRF, XSS, token hijacking, DoS attacks, and data breaches. This knowledge has equipped me with the necessary skills to implement security measures effectively and ensure the protection of sensitive data.The concept of authorisation has emerged as a crucial aspect of working with the Spotify API. Understanding the OAuth2 protocol and its different grant types, such as the authorisation code flow, has enabled me to develop secure applications that require valid client credentials for each request. By securely storing tokens and implementing parameterised queries, I have learned how to mitigate risks such as SQL injection attacks and unauthorised access. Overall, the evaluation of the Spotify Web API has helped me systematically develop and implement the skills required to be an effective member of a development team in a virtual professional environment. By adopting real-life perspectives on team roles and organisation, I have gained practical experience in data wrangling, security implementation, and adherence to industry standards and best practices.
</p>

									<h3>7. Back Up Procedure</h3>
									<h3> Learning Outcomes </h3>

								<ul>
									<li>Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling.</li>
									<li> Critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques (involving preparing, cleaning, exploring, creating, optimising and evaluating big data) to solve them.</li>
									<li>Design, develop and evaluate solutions for processing datasets and solving complex problems in various environments using relevant programming paradigms. </li>
								</ul>

									<h3> Evaluation of GFS Backup system </h3>

									<p> The Grandfather-Father-Son (GFS) backup procedure, a hierarchical model for preserving data, is particularly beneficial for large databases due to its strategic use of resources. The tiered structure of daily 'Son', weekly 'Father', and monthly 'Grandfather' backups helps avoid redundancy and optimises storage use. Daily backups capture essential modifications, whereas weekly and monthly backups provide more comprehensive overviews, effectively reducing the volume of duplicated data. As a result, it requires less storage space and processing power, making it more resource-efficient than many other methods (Vitanium, 2019).

However, its effectiveness compared to other methods depends on specific needs. For example, GFS may not suit businesses needing instantaneous, full data recovery as it relies on incremental backups. Continuous Data Protection (CDP), offering real-time backups, may serve such needs better, albeit at the cost of more resources. Therefore, the effectiveness of the GFS system hinges on balancing the particular requirements of an organisation against the resources available for data backup and recovery (LinkedIn, 2023).
</p>

									<p><b> References </b></p>
									<p> LinkedIn (2023) Grandfather-Father-Son Backup? Understanding the Classic Approach to Data Protection. Available from: https://www.linkedin.com/pulse/grandfather-father-son-backup-understanding-classic-approach-salameh/ [Accessed 21st July]. </p>]
									<p> Vitanium (2019) Backup Best Practices – The Grandfather – Father – Son Backup Strategy. Available from: https://vitanium.com/best-backup-practices-the-grandfather-father-son-backup-strategy/. [Accessed 21st July 2023]. </p>
							
									<h3> Reflections </h3>
									
									





									
									<h3>Meeting Recordings First4Aid Application </h3>
									<html>
										<head>
										   <title> <b>July 1st 2022</b></title>
										</head>
										<body>
										   <p><b>July 1st 2022</b></p>
										   <br />
										   <iframe width="560" height="315" src="https://www.youtube.com/embed/2LwrcfNxVG4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
										   </iframe>
										</body>
									 </html>
									 <html>
										<head>
										   <title><b>July 8th 2022</b></title>
										</head>
										<body>
										   <p><b>July 8th 2022</b></p>
										   <br />
										   <iframe width="560" height="315" src="https://www.youtube.com/embed/D4l6Aj-kSH0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
										   </iframe>
										</body>
									 </html>
									 <html>
										<head>
										   <title><b>July 15th 2022</b></title>
										</head>
										<body>
										   <p><b>July 15th 2022</b></p>
										   <br />
										   <iframe width="560" height="315" src="https://www.youtube.com/embed/U9plIwG8u8k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
										   </iframe>
										</body>
									 </html>
									 <html>
										<head>
										   <title><b>July 22nd 2022</b></title>
										</head>
										<body>
										   <p><b>July 22nd 2022</b></p>
										   <br />
										   <iframe width="560" height="315" src="https://www.youtube.com/embed/Ygurrk-tvxc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
										   </iframe>
										</body>
									 </html>
									 <html>
										<head>
										   <title><b>August 5th 2022</b></title>
										</head>
										<body>
										   <p><b>August 5th 2022</b></p>
										   <br />
										   <iframe width="560" height="315" src="https://www.youtube.com/embed/TTZ3WdEHPa8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
										   </iframe>
										</body>
									 </html>
									 <html>
										<head>
										   <title><b>August 12th 2022</b></title>
										</head>
										<body>
										   <p><b>August 12th 2022</b></p>
										   <br />
										   <iframe width="560" height="315" src="https://www.youtube.com/embed/2QNanpORjx8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
										   </iframe>
										</body>
									 </html>
									 <html>
										<head>
										   <title><b>August 19th 2022</b></title>
										</head>
										<body>
										   <p><b>August 19th 2022</b></p>
										   <br />
										   <iframe width="560" height="315" src="https://www.youtube.com/embed/FlPc7jBr2NY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
										   </iframe>
										</body>
									 </html>
									 <html>
										<head>
										   <title><b>August 22nd 2022</b></title>
										</head>
										<body>
										   <p><b>August 22nd 2022</b></p>
										   <br />
										   <iframe width="560" height="315" src="https://www.youtube.com/embed/lVJSqSlo_2A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
										   </iframe>
										</body>
									 </html>

					</div>

								</div>
							
							</section>

					</div>

				<!-- Contact -->
					<section id="contact">
						<div class="inner">
							<section>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<label for="name">Name</label>
											<input type="text" name="name" id="name" />
										</div>
										<div class="field half">
											<label for="email">Email</label>
											<input type="text" name="email" id="email" />
										</div>
										<div class="field">
											<label for="message">Message</label>
											<textarea name="message" id="message" rows="6"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send Message" class="primary" /></li>
										<li><input type="reset" value="Clear" /></li>
									</ul>
								</form>
							</section>
							<section class="split">
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-envelope"></span>
										<h3>Email</h3>
										<a href="#">tarapatricecasserly@gmail.com</a>
									</div>
								</section>
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-phone"></span>
										<h3>Phone</h3>
										<span>0000000</span>
									</div>
								</section>
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-home"></span>
										<h3>Address</h3>
										<span>LONDON<br />
										<br />
										</span>
									</div>
								</section>
							</section>
						</div>
					</section>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="icons">
								<li><a" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
								<li><a href="https://github.com/taracasserly?tab=repositories" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="https://www.linkedin.com/in/tarapcasserly/" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							</ul>
							
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
